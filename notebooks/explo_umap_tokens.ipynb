{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import math\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets.anorak import ANORAKFewShot\n",
    "from histo_utils.macenko_torch import (\n",
    "    normalize_and_unmix,\n",
    ")\n",
    "from models.histo_encoder import Uni2Encoder\n",
    "from models.histo_protonet_decoder import masks_to_token_hard_from_semantic\n",
    "from training.tiler import GridPadTiler, Tiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3dd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/valentin/workspaces/benchmark-vfm-ss/data/ANORAK_10x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compiler.disable\n",
    "def to_per_pixel_targets_semantic(\n",
    "    targets: list[dict],\n",
    "    ignore_idx: int,\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Convert list of instance masks into a single-channel semantic map per image.\"\"\"\n",
    "    out: list[torch.Tensor] = []\n",
    "    for t in targets:\n",
    "        h, w = t[\"masks\"].shape[-2:]\n",
    "        y = torch.full((h, w),\n",
    "                       ignore_idx,\n",
    "                       dtype=t[\"labels\"].dtype,\n",
    "                       device=t[\"labels\"].device)\n",
    "        for i, m in enumerate(t[\"masks\"]):\n",
    "            y[m] = t[\"labels\"][i]\n",
    "        out.append(y)  # [H,W] long\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657aed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _device_of(mod: torch.nn.Module) -> torch.device:\n",
    "    for p in mod.parameters():\n",
    "        return p.device\n",
    "    for b in mod.buffers():\n",
    "        return b.device\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_tokens_per_class(\n",
    "    encoder,  # your Uni2Encoder-like module returning [B,Q,D] for crops\n",
    "    dataloader,\n",
    "    *,\n",
    "    num_classes: int = 7,\n",
    "    ignore_idx: int = 255,\n",
    "    tile_size: int = 448,\n",
    "    stride: int = 448,\n",
    "    n_per_class: int = 2000,  # cap per class\n",
    "    bg_idx: int = 0,\n",
    "    include_background: bool = True,  # set False to drop class 0\n",
    "    purity_thresh: Optional[\n",
    "        float] = None,  # e.g. 0.9 to keep only ≥90% pure tokens\n",
    "    renorm_exclude_ignore: bool = True,\n",
    "    drop_background_only: bool = True,\n",
    "    progress: bool = True,\n",
    "    rng: Optional[torch.Generator] = None,  # for reproducibility\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, Dict[int, int], Dict[int, int]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X:  [M, D]   tokens (M ≤ num_classes * n_per_class)\n",
    "      y:  [M]      class ids (long)\n",
    "      seen_per_class: dict c-> count of valid tokens seen\n",
    "      kept_per_class: dict c-> count kept (≤ n_per_class)\n",
    "\n",
    "    Balanced sampling via per-class reservoir sampling.\n",
    "    \"\"\"\n",
    "    device = _device_of(encoder)\n",
    "    encoder.eval()\n",
    "\n",
    "    # tilers\n",
    "    img_tiler = GridPadTiler(tile=tile_size,\n",
    "                             stride=stride,\n",
    "                             weighted_blend=False,\n",
    "                             pad_mode=\"replicate\",\n",
    "                             pad_value=0.0)\n",
    "    tgt_tiler = GridPadTiler(tile=tile_size,\n",
    "                             stride=stride,\n",
    "                             weighted_blend=False,\n",
    "                             pad_mode=\"constant\",\n",
    "                             pad_value=float(ignore_idx))\n",
    "\n",
    "    # per-class reservoirs\n",
    "    buffers_X: Dict[int, List[torch.Tensor]] = {\n",
    "        c: []\n",
    "        for c in range(num_classes)\n",
    "    }\n",
    "    buffers_y: Dict[int, List[int]] = {c: [] for c in range(num_classes)}\n",
    "    seen_per_class: Dict[int, int] = {c: 0 for c in range(num_classes)}\n",
    "\n",
    "    # RNG\n",
    "    if rng is None:\n",
    "        rng = torch.Generator(device=\"cpu\")\n",
    "        rng.manual_seed(0)\n",
    "\n",
    "    it = tqdm(dataloader, desc=\"gather tokens\",\n",
    "              leave=False) if progress else dataloader\n",
    "    for imgs, targets in it:\n",
    "        # tile images\n",
    "        imgs, _, _, _ = normalize_and_unmix(imgs)\n",
    "        imgs = imgs.permute(0, 2, 3, 1)  # [B, 3, H,W]\n",
    "        crops, _, _ = img_tiler.window(imgs)  # [N,3,T,T]\n",
    "        crops = (crops.to(device) / 255.0)\n",
    "\n",
    "        # tile semantic targets as single-channel maps\n",
    "        sem_list = to_per_pixel_targets_semantic(targets,\n",
    "                                                 ignore_idx)  # list of [H,W]\n",
    "        sem_list = [y.unsqueeze(0) for y in sem_list]  # -> [1,H,W]\n",
    "        tgt_crops, _, _ = tgt_tiler.window(sem_list)  # [N,1,T,T]\n",
    "\n",
    "        # encoder -> tokens\n",
    "        tokens = encoder(crops)  # [N,Q,D]\n",
    "        N, Q, D = tokens.shape\n",
    "        X_flat = tokens.reshape(N * Q, D)  # [N*Q, D]\n",
    "\n",
    "        # hard labels per token on the grid\n",
    "        y_hard, valid = masks_to_token_hard_from_semantic(\n",
    "            tgt_crops.to(device),\n",
    "            num_classes=num_classes,\n",
    "            grid_size=encoder.grid_size,  # (Ht,Wt)\n",
    "            ignore_idx=ignore_idx,\n",
    "            bg_idx=bg_idx,\n",
    "            renorm_exclude_ignore=renorm_exclude_ignore,\n",
    "            drop_background_only=drop_background_only,\n",
    "            purity_thresh=purity_thresh,\n",
    "        )  # y_hard:[N,Q], valid:[N,Q] (bool)\n",
    "\n",
    "        y_flat = y_hard.reshape(-1)  # [N*Q]\n",
    "        m_flat = valid.reshape(-1)  # [N*Q]\n",
    "\n",
    "        # optional: drop background\n",
    "        if not include_background:\n",
    "            m_flat = m_flat & (y_flat != bg_idx)\n",
    "\n",
    "        # collect valid\n",
    "        if m_flat.any():\n",
    "            Xv = X_flat[m_flat]  # [K,D]\n",
    "            yv = y_flat[m_flat].to(\"cpu\")  # [K]\n",
    "            Xv = Xv.to(\"cpu\")  # keep CPU to save VRAM\n",
    "\n",
    "            # reservoir update per class\n",
    "            for cls in yv.unique().tolist():\n",
    "                idxs = (yv == cls).nonzero(as_tuple=False).squeeze(1)\n",
    "                cls_batch = Xv[idxs]  # [k_c, D]\n",
    "                for i in range(cls_batch.shape[0]):\n",
    "                    seen_per_class[cls] += 1\n",
    "                    buf = buffers_X[cls]\n",
    "                    if len(buf) < n_per_class:\n",
    "                        buf.append(cls_batch[i])\n",
    "                        buffers_y[cls].append(cls)\n",
    "                    else:\n",
    "                        # reservoir replacement\n",
    "                        j = int(\n",
    "                            torch.randint(0,\n",
    "                                          seen_per_class[cls], (1, ),\n",
    "                                          generator=rng).item())\n",
    "                        if j < n_per_class:\n",
    "                            buf[j] = cls_batch[i]\n",
    "                            buffers_y[cls][j] = cls\n",
    "\n",
    "    # stack results\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    kept_per_class = {}\n",
    "    for cls in range(num_classes):\n",
    "        kept_per_class[cls] = len(buffers_X[cls])\n",
    "        if kept_per_class[cls] > 0:\n",
    "            X_out.append(torch.stack(buffers_X[cls], dim=0))  # [m_c, D]\n",
    "            y_out.append(torch.tensor(buffers_y[cls],\n",
    "                                      dtype=torch.long))  # [m_c]\n",
    "\n",
    "    if len(X_out) == 0:\n",
    "        return (torch.empty(0, encoder.embed_dim),\n",
    "                torch.empty(0, dtype=torch.long), seen_per_class, {\n",
    "                    c: 0\n",
    "                    for c in range(num_classes)\n",
    "                })\n",
    "\n",
    "    X = torch.cat(X_out, dim=0)\n",
    "    y = torch.cat(y_out, dim=0)\n",
    "    return X, y, seen_per_class, kept_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ANORAKFewShot(\n",
    "    root_dir,\n",
    "    devices=1,\n",
    "    num_workers=0,\n",
    "    fold=0,\n",
    "    img_size=(448, 448),\n",
    "    batch_size=1,\n",
    "    num_classes=7,\n",
    "    ignore_idx=255,\n",
    ")\n",
    "dm.setup(\"fit\")\n",
    "train_loader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Uni2Encoder()\n",
    "device = torch.device(\"cuda:0\")\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, seen, kept = sample_tokens_per_class(\n",
    "    encoder=encoder.eval(),\n",
    "    dataloader=train_loader,              # or train_loader\n",
    "    num_classes=7,\n",
    "    ignore_idx=255,\n",
    "    tile_size=448,\n",
    "    stride=448,\n",
    "    n_per_class=3000,\n",
    "    include_background=True,           # often nicer for histo viz\n",
    "    purity_thresh=0.9,                  # require ≥90% pure tokens\n",
    ")\n",
    "\n",
    "print(\"per-class seen:\", seen)\n",
    "print(\"per-class kept:\", kept)\n",
    "print(\"X\", X.shape, \"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "# or: from sklearn.manifold import TSNE\n",
    "\n",
    "umap = UMAP(\n",
    "    n_neighbors=10,\n",
    "    min_dist=0.05,\n",
    "    spread=1.5,\n",
    "    metric=\"cosine\",\n",
    "    random_state=0,\n",
    "    n_jobs=24,\n",
    ")\n",
    "Z = umap.fit_transform(X.numpy())  # [M,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e490f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ce54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Create 7 discrete colors (for classes 0–6)\n",
    "cmap = ListedColormap(plt.cm.tab10.colors[:7])\n",
    "\n",
    "# Define bin edges centered on integers 0–6\n",
    "bounds = np.arange(-0.5, 7.5, 1)  # [-0.5, 0.5, 1.5, ..., 6.5]\n",
    "norm = BoundaryNorm(bounds, cmap.N, clip=True)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sc = plt.scatter(Z[:, 0], Z[:, 1], c=y.numpy(), s=3, cmap=cmap, norm=norm)\n",
    "cbar = plt.colorbar(sc, ticks=np.arange(0, 7))\n",
    "cbar.set_label(\"Class\")\n",
    "plt.title(\"UMAP of spatial tokens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd6eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-vfm-ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
