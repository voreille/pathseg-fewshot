{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torchvision.transforms import CenterCrop\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Add the parent directory to path so we can import the modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the model classes and data modules from the project\n",
    "from training.mask2former_semantic import Mask2formerSemantic\n",
    "from datasets.ade20k import ADE20K\n",
    "from models.mask2former_decoder import  ModifiedMask2formerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model from checkpoint\n",
    "\n",
    "checkpoint_path = \"/home/valentin/workspaces/benchmark-vfm-ss/data/lightning_logs/syop5eg0/checkpoints/epoch=31-step=40000.ckpt\"\n",
    "\n",
    "# For Mask2Former model (if you trained with mask2former config)\n",
    "model = Mask2formerSemantic.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    # You need to provide the network again since it's not saved in the checkpoint\n",
    "    network=ModifiedMask2formerDecoder(\n",
    "        img_size=(512, 512),\n",
    "        num_classes=150,  # ADE20K has 150 classes\n",
    "        encoder_name=\"vit_base_patch14_dinov2\",  # Or your encoder\n",
    "    ),\n",
    "    strict=False  # In case of minor mismatches\n",
    ")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "print(f\"Model loaded from {checkpoint_path}\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79806efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        return x.to(device, non_blocking=True)\n",
    "    if isinstance(x, dict):\n",
    "        return {k: move_to_device(v, device) for k, v in x.items()}\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return type(x)(move_to_device(v, device) for v in x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data module to load validation data\n",
    "data_module = ADE20K(\n",
    "    root=\"../data\",  # Adjust path to your data\n",
    "    devices=1,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    img_size=(512, 512),\n",
    "    num_classes=150,\n",
    "    num_metrics=1\n",
    ")\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d88057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a sample from validation set\n",
    "val_dataloader = data_module.val_dataloader()\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "\n",
    "# Extract image and target\n",
    "sample_batch = move_to_device(sample_batch, model.device)\n",
    "img, target = sample_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce179a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0][\"masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_per_pixel_labels(target):\n",
    "    masks = target[\"masks\"]\n",
    "    labels = target[\"labels\"]\n",
    "    return torch.einsum(\"nmk,n->mk\", masks.float(), labels.float()).long()\n",
    "\n",
    "\n",
    "def to_per_pixel_logit(mask_logits, class_logits, query_idx=None):\n",
    "    if query_idx is not None:\n",
    "        mask_logits = mask_logits[:, query_idx, :, :]\n",
    "        class_logits = class_logits[:, query_idx, :]\n",
    "    return torch.einsum(\n",
    "        \"bqhw,bqc->bchw\",\n",
    "        mask_logits.sigmoid(),\n",
    "        class_logits.softmax(dim=-1)[..., :-1],\n",
    "    )\n",
    "\n",
    "def to_query_per_pixels(query_embeddings, pixel_embeddings):\n",
    "    return torch.einsum(\"qc,bchw->bqhw\", query_embeddings, pixel_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c89ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the sample\n",
    "with torch.no_grad():\n",
    "    # Use the model's eval_step method which handles the full pipeline\n",
    "    logits = model.eval_step(sample_batch, batch_idx=0, is_notebook=True)\n",
    "    \n",
    "    # Get predictions by taking argmax\n",
    "    predictions = torch.argmax(logits[0], dim=0)\n",
    "    \n",
    "    print(f\"Logits shape: {logits[0].shape}\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Predicted classes: {torch.unique(predictions)}\")\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    img_np = img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    target_np = to_per_pixel_labels(target[0]).cpu().numpy()\n",
    "    pred_np = predictions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06951252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results using the model's built-in plotting function\n",
    "# This leverages the same visualization code used during training\n",
    "\n",
    "plot_image = model.plot_semantic(\n",
    "    img[0],           # Original image\n",
    "    to_per_pixel_labels(target[0]),        # Ground truth segmentation \n",
    "    logits=logits[0]  # Model predictions\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(plot_image)\n",
    "plt.axis('off')\n",
    "plt.title('Left: Original Image | Middle: Ground Truth | Right: Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f244af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torch.stack(img, axis=0)\n",
    "test_image = CenterCrop((512, 512))(test_image)\n",
    "\n",
    "test_target = torch.stack([to_per_pixel_labels(t) for t in target], axis=0)\n",
    "test_target = CenterCrop((512, 512))(test_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a83ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image[0, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    test_output = model.network.forward_dict(test_image / 255.0)\n",
    "test_output = move_to_device(test_output, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits_per_layer = test_output[\"mask_logits_per_layer\"]\n",
    "class_logits_per_layer = test_output[\"class_logits_per_layer\"]\n",
    "mask_embeddings_per_layer = test_output[\"mask_embeddings_per_layer\"]\n",
    "per_pixel_embeddings = test_output[\"per_pixel_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_embeddings_per_layer[-1].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits_per_layer[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_logits_per_layer[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_pixel_embeddings.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.network.q.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_per_pixels = to_query_per_pixels(model.network.q.weight.cpu(), per_pixel_embeddings).detach().cpu().numpy().squeeze()\n",
    "mask_embeddings_per_pixels_l0 = to_query_per_pixels(mask_embeddings_per_layer[0].squeeze(), per_pixel_embeddings).detach().cpu().numpy().squeeze()\n",
    "mask_embeddings_per_pixels_lf = to_query_per_pixels(mask_embeddings_per_layer[-1].squeeze(), per_pixel_embeddings).detach().cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68978464",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20\n",
    "indices = np.random.choice(query_per_pixels.shape[0], n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=n, ncols=3, figsize=(6, 3*n))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i, 0].imshow(query_per_pixels[idx], cmap=\"gray\")\n",
    "    axes[i, 0].set_title(f\"Query {idx}\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(mask_embeddings_per_pixels_l0[idx], cmap=\"gray\")\n",
    "    axes[i, 1].set_title(f\"Mask embeddings L0 for query {idx}\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    axes[i, 2].imshow(mask_embeddings_per_pixels_lf[idx], cmap=\"gray\")\n",
    "    axes[i, 2].set_title(f\"Mask embeddings Lf for query {idx}\")\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fe63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_per_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits = F.interpolate(mask_logits_per_layer[-1], (512, 512), mode=\"bilinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee011851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_logits = class_logits_per_layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3accc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx = np.random.choice(class_logits.shape[1], size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logits = to_per_pixel_logit(mask_logits, class_logits, query_idx=query_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image = model.plot_semantic(\n",
    "    test_image[0, ...],  # Original image\n",
    "    test_target[0, ...],  # Ground truth segmentation \n",
    "    logits=test_logits[0, ...]  # Model predictions\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(plot_image)\n",
    "plt.axis('off')\n",
    "plt.title('Left: Original Image | Middle: Ground Truth | Right: Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb661f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35fe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-vfm-ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
