{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torchvision.transforms import CenterCrop\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Add the parent directory to path so we can import the modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the model classes and data modules from the project\n",
    "from training.linear_semantic import LinearSemantic\n",
    "from datasets.anorak import ANORAK\n",
    "from models.histo_linear_decoder import LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model from checkpoint\n",
    "\n",
    "checkpoint_path = \"/home/valentin/workspaces/benchmark-vfm-ss/data/lightning_logs/6t30deru/checkpoints/epoch=1289-step=39990.ckpt\"\n",
    "\n",
    "# For Mask2Former model (if you trained with mask2former config)\n",
    "model = LinearSemantic.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    # You need to provide the network again since it's not saved in the checkpoint\n",
    "    network=LinearDecoder(\n",
    "        img_size=(448, 448),\n",
    "        encoder_name=\"hf-hub:MahmoodLab/UNI2-h\",\n",
    "        num_classes=7,\n",
    "    ),\n",
    "    strict=False  # In case of minor mismatches\n",
    ")\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "print(f\"Model loaded from {checkpoint_path}\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79806efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(x, device):\n",
    "    if torch.is_tensor(x):\n",
    "        return x.to(device, non_blocking=True)\n",
    "    if isinstance(x, dict):\n",
    "        return {k: move_to_device(v, device) for k, v in x.items()}\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return type(x)(move_to_device(v, device) for v in x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data module to load validation data\n",
    "data_module = ANORAK(\n",
    "    root=\"../data/ANORAK\",  # Adjust path to your data\n",
    "    devices=1,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    img_size=(448, 448),\n",
    "    num_classes=7,\n",
    "    num_metrics=1\n",
    ")\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.ignore_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ff488",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.compute_class_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d88057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a sample from validation set\n",
    "val_dataloader = data_module.val_dataloader()\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "\n",
    "# Extract image and target\n",
    "sample_batch = move_to_device(sample_batch, model.device)\n",
    "img, target = sample_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce179a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0][\"masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fad4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_semantic(img[0], target[0][\"masks\"], target[0][\"labels\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac292561",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    logits = model.eval_step(sample_batch, is_notebook=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5bd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_logits = np.transpose(logits[0].cpu().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.argmax(np_logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf62b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-vfm-ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
